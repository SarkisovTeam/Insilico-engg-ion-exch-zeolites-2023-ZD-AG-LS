{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Catio distribution across sites in LTA\n","\n","This notebook detects the location of the cation across many configurations in a RASPA output PDB file and shows the average vallue across the different sites. The locations of the different sites are defined based on the type of the cages between which they exist. There are three types of pockets in LTA\n","\n","- The sodalite cage\n","- Supercage\n","- The cubical cubical pocket.\n","\n","Between which there exist three types of sites as shown in the figure below. \n","\n","![LTA sites](./structure-siting.jpg)\n","\n","Please refer to the original paper for more details on the algorithm and context\n","\n","[In Silico Engineering of Ion-Exchanged Zeolites for High-Performance Carbon Capture in Psa Processes](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4306028)\n","\n","\n","Replace the Movie file in the repo with yours to detect the cation distribution in your LTA configurations."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Define functions"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["\n","def AABB_on_atoms(ase_atoms, points=None):\n","\n","    from freud.locality import AABBQuery\n","    from freud.box import Box\n","    import ase \n","    # from ase.data import vdw_radii\n","\n","\n","    if points is None:\n","        points = ase_atoms.get_positions()\n","    cell = ase.geometry.complete_cell(ase_atoms.get_cell()).T # this needs to be an upper triangular matrix, essential column vectors, not row vectors\n","    \n","    box = Box.from_matrix(cell, dimensions =3) # Define the box for Freud\n","    ab  = AABBQuery(box=box, points=points) # Compute the AABB Tree\n","    # Perform the nearest neighbor query with n-neighbors (10 by default)\n","        \n","    return ab\n","def get_wall_windows2(regions, maxima, dgrid, ase_atoms, wall_thickness=2):\n","\n","    import numpy as np\n","    from icecream import ic\n","    wall_windows = []\n","    shape =regions.shape\n","    dinterp = interpolate_labels(dgrid)\n","    for a in [0,1,2]:\n","        ic.disable()\n","        ic(a)\n","        llr = regions.take(indices=0 ,axis=a) #* left wall\n","        rrr = regions.take(indices=shape[a]-1 ,axis=a)  #* right wall\n","\n","        map_list = np.unique(list(zip(llr.flatten(), rrr.flatten())), axis=0)\n","        map_list = np.vstack([m for m in map_list if np.all(m!=0)])\n","        flags = np.vstack([[maxima[k - 1][a]>wall_thickness, maxima[v-1][a]<(shape[a]-wall_thickness)] for k,v in map_list])    \n","        flags = np.logical_and(flags[:,0],flags[:,1])\n","        # window_indices = [np.vstack(np.where(llr==m[0])).mean(axis=1) for i,m in enumerate(map_list) if flags[i]]\n","        wregs_l  = [(np.array([m[0], m[1]]), True,'periodic') for i, m in enumerate(map_list) if flags[i]]\n","        wregs_r  = [(np.array([m[1], m[0]]), True,'periodic') for i, m in enumerate(map_list) if flags[i]]\n","        window_indices = [np.vstack(np.where(llr==m[0])).mean(axis=1).T for i, m in enumerate(map_list) if flags[i]]\n","        ic(window_indices)\n","        if np.any(flags):\n","            window_indices_l = np.insert(np.vstack(window_indices), [a], np.zeros((len(window_indices),1)), axis=1  )\n","            window_indices_r = np.insert(np.vstack(window_indices), [a], (shape[a] -1)*np.ones((len(window_indices),1)), axis=1 )\n","            window_radii_l = np.vstack(dinterp(window_indices_l/regions.shape))\n","            window_radii_r = np.vstack(dinterp(window_indices_r/regions.shape))\n","            window_coords_l = find_coord_from_indices(window_indices_l, shape=shape, ase_atoms=ase_atoms)\n","            window_coords_r = find_coord_from_indices(window_indices_r, shape=shape, ase_atoms=ase_atoms)\n","            ic(window_coords_l)\n","            ic(window_coords_r)\n","            window_rows_l = np.vstack([[*w, np.array([ window_coords_l[i] ]), window_radii_l[i]] for i,w in enumerate(wregs_l)])\n","            window_rows_r = np.vstack([[*w, np.array([ window_coords_r[i] ]), window_radii_r[i]] for i,w in enumerate(wregs_r)])\n","            wall_windows.append(window_rows_l) \n","            wall_windows.append(window_rows_r) \n","            \n","    if len(wall_windows)>0:\n","        wall_windows =  np.vstack(wall_windows)\n","    return wall_windows\n","def get_connections_for_rag(rag,region_labels, maxima,ase_atoms,dgrid, minimum_window_separation=1.0, wall_windows=True, wall_thickness=2):\n","    \n","\n","    def find_windows(regs):\n","        \n","\n","            import numpy as np\n","            from sklearn.cluster import AgglomerativeClustering\n","            i = regs[0]\n","            j = regs[1]\n","            check_flag = np.logical_or(np.logical_and(region_labels== i, outer.get(j)), np.logical_and(region_labels== j, outer.get(i)))\n","            if np.sum(check_flag) > 0:\n","                # print(\"Edges found between \"+str(i)+' and '+ str(j) + ' sum: '+str(np.sum(check_flag)))\n","                windices = np.vstack(np.where(check_flag)).T  # * (N,3)\n","                window_points = np.dot(A_unit, (windices /region_labels.shape).T).T  # * (N,3)\n","                # ic(window_points)\n","                # * No clustering\n","                # window_centers = np.mean(window_points, axis=0)\n","                \n","                # * DBScan\n","                # X_scaled = StandardScaler().fit_transform(window_points)\n","                # db = DBSCAN(eps=0.5, min_samples=10).fit(X_scaled)\n","                # window_centers = np.vstack([window_points[db.labels_==i].mean(axis=0) for i in np.unique(db.labels_) if i != -1])\n","                \n","                # if len(window_points) >1:\n","                # * Agglomerative\n","                ag = AgglomerativeClustering(n_clusters=None, linkage='single',distance_threshold=minimum_window_separation, compute_full_tree=True).fit(window_points)\n","                window_centers = np.vstack([window_points[ag.labels_==i].mean(axis=0) for i in np.unique(ag.labels_) if i != -1])\n","            # else:\n","                # window_centers = window_points\n","                window_fractional = get_fractional_coordinates(window_centers,ase_atoms)\n","                window_radii = np.array(dinterp(window_fractional))\n","                return np.array([regs, True,'internal', window_centers, window_radii], dtype='object') # returns a list of [[r1,r2], connected or not, array of window_centers]\n","            else:\n","                return np.array([regs, False,'internal', None, None], dtype='object')\n","\n","    #* find the internal windows\n","    from skimage.segmentation import find_boundaries\n","    import ase\n","    import numpy as np\n","    from rich.progress import track\n","    connected_regions = np.vstack(list(rag.edges()))\n","    list_of_region_indices = np.delete(np.unique(region_labels),0)\n","    outer = {reg:find_boundaries(region_labels== reg, mode='outer') for reg in list_of_region_indices}\n","    dinterp = interpolate_labels(dgrid)\n","    minimum_window_separation = 1.0\n","    A_unit = ase.geometry.complete_cell(ase_atoms.get_cell()).T    \n","\n","    connections= [find_windows(regs) for regs in track(rag.edges, description='Finding windows:')]\n","\n","    #* Windows on the wall are to be calculated (periodic windows)\n","    if wall_windows:\n","        wwl = get_wall_windows2(regions=region_labels, maxima=maxima, dgrid=dgrid, ase_atoms=ase_atoms, wall_thickness=wall_thickness)\n","        if len(wwl)>0:\n","            connections = connections+wwl.tolist()\n","    \n","    return connections\n","\n","# * Assign ions to cages\n","def find_coord_from_indices(indices, shape, ase_atoms):\n","\n","    import ase\n","    import numpy as np\n","    cell = ase.geometry.complete_cell(ase_atoms.get_cell()).T\n","    return np.dot(cell, (indices/shape).T).T\n","def get_cage_for_ions(coords, cage_labels, interpolator, ase_atoms):\n","\n","    import numpy as np\n","    output_labels = []\n","    for coord in coords:\n","        # print(coords.shape)\n","        # print((interpolator(get_fractional_coordinates(coords, ase_atoms=nax)).astype(int)-1).shape)\n","        try:\n","            output_labels.append(np.array(cage_labels)[interpolator(mgr.get_fractional_coordinates(coord, ase_atoms=ase_atoms)).astype(int)-1])\n","        except:\n","            print(np.max(coord))\n","            print(np.min(coord))\n","            pass\n","    return np.vstack(output_labels)\n","def gpd_aabb(grid_points, ase_atoms, radii, n_neighbors=10, probe_radius=0.0):\n","\n","    from freud.locality import AABBQuery\n","    from freud.box import Box\n","    import ase \n","    # from ase.data import vdw_radii\n","    from icecream import ic\n","    import pandas as pd\n","    import numpy as np\n","\n","    framework_atom_positions = ase_atoms.get_positions()\n","    cell = ase.geometry.complete_cell(ase_atoms.get_cell()).T # this needs to be an upper triangular matrix, essential column vectors, not row vectors\n","    \n","    box = Box.from_matrix(cell, dimensions =3) # Define the box for Freud\n","    ab  = AABBQuery(box=box, points=framework_atom_positions) # Compute the AABB Tree\n","    # Perform the nearest neighbor query with n-neighbors (10 by default)\n","    ab_r = ab.query(grid_points, query_args=dict(mode='nearest', num_neighbors=n_neighbors, exclude_ii=True))\n","    ab_r_nl = ab_r.toNeighborList(sort_by_distance=True) #convert query to neighborlist\n","    nls = ab_r_nl.point_indices.reshape(-1,n_neighbors)  # get the indices of the neighbors for each grid pt. to lookup the radii later\n","    dists =ab_r_nl.distances.reshape(-1,n_neighbors) # get the distances to the neighbors of each grid pt.\n","    # print(ab, ab_r, ab_r_nl, nls, [radii[nl] for nl in nls])#, np.min(dists - [radii[nl] for nl in nls],axis=1))\n","    # return the minimum (shortest distance to the surface) in each row\n","    return np.min(dists - [radii[nl] for nl in nls],axis=1)\n","\n","def dask_grid_over_atoms(ase_atoms, spacing=0.1, chunksize=50000):\n","\n","    import ase\n","    import dask.array as da\n","    import numpy as np\n","    #number of grid points in each direction \n","    [nx, ny, nz] = (ase_atoms.get_cell_lengths_and_angles()[0:3] / spacing).astype(int) + 1\n","    gpoints = (da.stack(da.meshgrid(np.linspace(0, 1, nx), np.linspace(0,1, ny), np.linspace(0,1, nz), indexing='ij'),-1).reshape(-1, 3)).rechunk(chunksize,3)\n","    cell = ase.geometry.complete_cell(ase_atoms.get_cell()).T # cell matrix\n","    return da.dot(cell, gpoints.T).T, (nx,ny,nz) #return the actual coordinates\n","\n","def dgrid_from_atoms(ase_atoms, radii=None, spacing=0.25, block_size=50000, n_neighbors=10, probe_radius=0):\n"," \n","    import numpy as np\n","    import dask.array as da\n","    from rich.progress import track\n","    gpoints, shape = dask_grid_over_atoms(ase_atoms, spacing=spacing, chunksize=block_size) #mesh grid\n","    dgrid = []\n","    import pandas as  pd \n","    if radii is None:\n","        vdw_radii = pd.Series({'H': 110.00000000000001,'He': 140.0,'Li': 182.0,'Be': 153.0,'B': 192.0,'C': 170.0,'N': 155.0,'O': 152.0,'F': 147.0,'Ne': 154.0,'Na': 227.0,'Mg': 173.0,'Al': 184.0,'Si': 210.0,'P': 180.0,'S': 180.0,'Cl': 175.0,'Ar': 188.0,'K': 275.0,'Ca': 231.0,'Sc': 215.0,'Ti': 211.0,'V': 206.99999999999997,'Cr': 206.0,'Mn': 204.99999999999997,'Fe': 204.0,'Co': 200.0,'Ni': 197.0,'Cu': 196.0,'Zn': 200.99999999999997,'Ga': 187.0,'Ge': 211.0,'As': 185.0,'Se': 190.0,'Br': 185.0,'Kr': 202.0,'Rb': 303.0,'Sr': 249.00000000000003,'Y': 231.99999999999997,'Zr': 223.0,'Nb': 218.00000000000003,'Mo': 217.0,'Tc': 216.0,'Ru': 213.0,'Rh': 210.0,'Pd': 210.0,'Ag': 211.0,'Cd': 218.00000000000003,'In': 193.0,'Sn': 217.0,'Sb': 206.0,'Te': 206.0,'I': 198.0,'Xe': 216.0,'Cs': 343.0,'Ba': 268.0,'La': 243.00000000000003,'Ce': 242.0,'Pr': 240.0,'Nd': 239.0,'Pm': 238.0,'Sm': 236.0,'Eu': 235.0,'Gd': 234.0,'Tb': 233.0,'Dy': 231.0,'Ho': 229.99999999999997,'Er': 229.0,'Tm': 227.0,'Yb': 225.99999999999997,'Lu': 224.00000000000003,'Hf': 223.0,'Ta': 222.00000000000003,'W': 218.00000000000003,'Re': 216.0,'Os': 216.0,'Ir': 213.0,'Pt': 213.0,'Au': 214.0,'Hg': 223.0,'Tl': 196.0,'Pb': 202.0,'Bi': 206.99999999999997,'Po': 197.0,'At': 202.0,'Rn': 220.00000000000003,'Fr': 348.0,'Ra': 283.0,'Ac': 247.00000000000003,'Th': 245.00000000000003,'Pa': 243.00000000000003,'U': 241.0,'Np': 239.0,'Pu': 243.00000000000003,'Am': 244.0,'Cm': 245.00000000000003,'Bk': 244.0,'Cf': 245.00000000000003,'Es': 245.00000000000003,'Fm': 245.00000000000003,'Md': 246.0,'No': 246.0,'Lr': 246.0,'Rf': np.nan,'Db': np.nan,'Sg': np.nan,'Bh': np.nan,'Hs': np.nan,'Mt': np.nan,'Ds': np.nan,'Rg': np.nan,'Cn': np.nan,'Nh': np.nan,'Fl': np.nan,'Mc': np.nan,'Lv': np.nan,'Ts': np.nan,'Og': np.nan})\n","        radii = vdw_radii[ase_atoms.get_chemical_symbols()].values/100.0\n","    \n","    for block in track(gpoints.blocks, description='Computing distance grid', total=gpoints.npartitions):\n","        dgrid.append(gpd_aabb(block.compute(), ase_atoms, radii, n_neighbors=n_neighbors,probe_radius=probe_radius )) #compute grid\n","    return da.hstack(dgrid).rechunk(chunks=block_size).reshape(shape) #return in correct shape\n","\n","\n","def get_rag_from_regions_and_grid(regions, dgrid, ase_atoms, maxima=None):\n"," \n","    \n","    import numpy as np\n","    # * compute a region adjacency graph from skimage\n","    from skimage.future import graph \n","    gr_mean = graph.rag_mean_color(dgrid, regions)\n","    \n","    from skimage.measure import regionprops \n","    # * add the centroid property to the rag nodes\n","    region_props = regionprops(regions)\n","    for region_prop in region_props:\n","        #add the coordinates of the centroid to the node\n","        gr_mean.nodes[region_prop['label']]['centroid'] = find_coord_from_indices( indices = np.array(region_prop['centroid']), ase_atoms=ase_atoms,shape=dgrid.shape)\n","        gr_mean.nodes[region_prop['label']]['centroid_indices'] = region_prop['centroid'] # * add the indices of the centroid to the node\n","    gr_mean.remove_node(0)\n","    if maxima is not None:\n","        maxima_radii = dgrid[tuple(maxima.T)] # * radii of the maxima\n","        gr_mean = add_maxima_to_rag(gr_mean, maxima, maxima_radii, regions.shape, ase_atoms)\n","        for edge in gr_mean.edges():\n","            gr_mean.edges[edge]['weight'] = np.linalg.norm(gr_mean.nodes[edge[0]]['maxima'] - gr_mean.nodes[edge[1]]['maxima'])\n","    else:\n","        for edge in gr_mean.edges():\n","            gr_mean.edges[edge]['weight'] = np.linalg.norm(gr_mean.nodes[edge[0]]['centroid'] - gr_mean.nodes[edge[1]]['centroid'])\n","    \n","    return gr_mean\n","\n","def regions_from_dgrid(dgrid, mask_thickness=1, h=0.2, min_distance=2, compactness=0):\n","  \n","    from scipy import ndimage as ndi\n","    import numpy as np\n","    import sparse\n","    # from .helper import interpolate_labels, find_coord_from_indices\n","    from skimage import segmentation\n","    from skimage.morphology import extrema\n","    from skimage.feature import peak_local_max, corner_peaks\n","    from sklearn.cluster import AgglomerativeClustering\n","    # peak_min =2\n","    # max_indices = np.vstack(np.where(extrema.h_maxima(dgrid, h=0.5)))\n","    # max_indices = np.vstack(np.where(extrema.local_maxima(dgrid)))\n","    # max_indices = np.array(peak_local_max(dgrid, **kwargs)).T\n","    # max_indices = np.array(peak_local_max(dgrid, **kwargs)).T\n","    # Finding the local maxima of the corner response function.\n","    \n","    # ag = AgglomerativeClustering(n_clusters=None, linkage='average', distance_threshold=np.max(np.array(dgrid.shape))/5, compute_full_tree=True).fit(max_indices)\n","    # max_indices= np.vstack([max_indices[ag.labels_==i].mean(axis=0) for i in np.unique(ag.labels_) if i != -1]).astype(int)\n","    # print(max_indices)\n","    # markers_sparse = sparse.COO(max_indices, 1, shape=dgrid.shape)\n","    from icecream import ic \n","    probe = mask_thickness\n","    import dask.array as da\n","    import warnings\n","    # reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int))\n","\n","    # max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1, **kwargs)).T\n","    # max_indices = np.array(peak_local_max(dgrid,  **kwargs)).T\n","    max_indices = np.vstack(np.where(extrema.h_maxima(dgrid,  h=h)))\n","    markers_sparse = sparse.COO(max_indices, 1, shape=dgrid.shape)\n","\n","    probe = mask_thickness\n","    import dask.array as da\n","    reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int), compactness=compactness)\n","\n","\n","\n","    # print(keep_these)\n","    # new_max_indices = max_indices[keep_these-1].T\n","\n","    new_max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1,p_norm=2)).T\n","    if len(new_max_indices.T) < len(max_indices.T):\n","        warnings.warn('Some maxima excluded as they fell inside the mask_thickness')\n","        # warnings.warn('Some maxima excluded as they the regions were less that 0.001 of the box')\n","    \n","    markers_sparse = sparse.COO(new_max_indices, 1, shape=dgrid.shape)\n","\n","    probe = mask_thickness\n","    import dask.array as da\n","    reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int), compactness=compactness)\n","\n","    new_max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1, p_norm=2)).T\n","    # ir = interpolate_labels(reg_labels)\n","    # new_max_indices = (new_max_indices.T[np.argsort(ir(new_max_indices.T/dgrid.shape))]).T\n","    \n","    # # new_max_indices = []\n","    # old_labels = np.unique(reg_labels)  # This is sorted aalreadylready\n","    # number_of_regions = len(old_labels)\n","    # new_labels = range(number_of_regions)\n","\n","    # # * Replace the old-labels with new\n","    # for i in range(number_of_regions):\n","    #     reg_labels[reg_labels== old_labels[i]] = new_labels[i]\n","\n","    # # *  Lets clean out the regions that are too cubical \n","    # props = regionprops(reg_labels)\n","    # print([(p.label, p.area/dgrid.size) for p in props])\n","    # len([(p.label, p.area/dgrid.size) for p in props if p.area > dgrid.size/1000])\n","    # keep_these = [p.label for p in props if p.area > dgrid.size/1000]\n","    # maxima2 = np.vstack([new_max_indices.T[k-1] for k in keep_these])\n","    # markers_sparse = sparse.COO(maxima2.T, 1, shape=dgrid.shape)\n","\n","    # # probe = mask_thickness\n","    # # import dask.array as da\n","    # reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int), compactness=False)\n","    # new_max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1)).T\n","\n","    #* Now interpolate the region labels and sort the maxima in the correct order    \n","    sort_index = np.argsort([reg_labels[tuple(m)] for m in new_max_indices.T])\n","    new_max_indices = (new_max_indices.T[sort_index]).T\n"," \n","\n","    return reg_labels, new_max_indices.T\n","def regions_from_dgrid(dgrid, mask_thickness=1, h=0.2, min_distance=2, compactness=0):\n","\n","    from scipy import ndimage as ndi\n","    import numpy as np\n","    import sparse\n","    # from .helper import interpolate_labels, find_coord_from_indices\n","    from skimage import segmentation\n","    from skimage.morphology import extrema\n","    from skimage.feature import peak_local_max, corner_peaks\n","    from sklearn.cluster import AgglomerativeClustering\n","    # peak_min =2\n","    # max_indices = np.vstack(np.where(extrema.h_maxima(dgrid, h=0.5)))\n","    # max_indices = np.vstack(np.where(extrema.local_maxima(dgrid)))\n","    # max_indices = np.array(peak_local_max(dgrid, **kwargs)).T\n","    # max_indices = np.array(peak_local_max(dgrid, **kwargs)).T\n","    # Finding the local maxima of the corner response function.\n","    \n","    # ag = AgglomerativeClustering(n_clusters=None, linkage='average', distance_threshold=np.max(np.array(dgrid.shape))/5, compute_full_tree=True).fit(max_indices)\n","    # max_indices= np.vstack([max_indices[ag.labels_==i].mean(axis=0) for i in np.unique(ag.labels_) if i != -1]).astype(int)\n","    # print(max_indices)\n","    # markers_sparse = sparse.COO(max_indices, 1, shape=dgrid.shape)\n","    from icecream import ic \n","    probe = mask_thickness\n","    import dask.array as da\n","    import warnings\n","    # reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int))\n","\n","    # max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1, **kwargs)).T\n","    # max_indices = np.array(peak_local_max(dgrid,  **kwargs)).T\n","    max_indices = np.vstack(np.where(extrema.h_maxima(dgrid,  h=h)))\n","    markers_sparse = sparse.COO(max_indices, 1, shape=dgrid.shape)\n","\n","    probe = mask_thickness\n","    import dask.array as da\n","    reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int), compactness=compactness)\n","\n","\n","\n","    # print(keep_these)\n","    # new_max_indices = max_indices[keep_these-1].T\n","\n","    new_max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1,p_norm=2)).T\n","    if len(new_max_indices.T) < len(max_indices.T):\n","        warnings.warn('Some maxima excluded as they fell inside the mask_thickness')\n","        # warnings.warn('Some maxima excluded as they the regions were less that 0.001 of the box')\n","    \n","    markers_sparse = sparse.COO(new_max_indices, 1, shape=dgrid.shape)\n","\n","    probe = mask_thickness\n","    import dask.array as da\n","    reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int), compactness=compactness)\n","\n","    new_max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1, p_norm=2)).T\n","    # ir = interpolate_labels(reg_labels)\n","    # new_max_indices = (new_max_indices.T[np.argsort(ir(new_max_indices.T/dgrid.shape))]).T\n","    \n","    # # new_max_indices = []\n","    # old_labels = np.unique(reg_labels)  # This is sorted aalreadylready\n","    # number_of_regions = len(old_labels)\n","    # new_labels = range(number_of_regions)\n","\n","    # # * Replace the old-labels with new\n","    # for i in range(number_of_regions):\n","    #     reg_labels[reg_labels== old_labels[i]] = new_labels[i]\n","\n","    # # *  Lets clean out the regions that are too cubical \n","    # props = regionprops(reg_labels)\n","    # print([(p.label, p.area/dgrid.size) for p in props])\n","    # len([(p.label, p.area/dgrid.size) for p in props if p.area > dgrid.size/1000])\n","    # keep_these = [p.label for p in props if p.area > dgrid.size/1000]\n","    # maxima2 = np.vstack([new_max_indices.T[k-1] for k in keep_these])\n","    # markers_sparse = sparse.COO(maxima2.T, 1, shape=dgrid.shape)\n","\n","    # # probe = mask_thickness\n","    # # import dask.array as da\n","    # reg_labels = segmentation.watershed(-dgrid, ndi.label(markers_sparse.todense())[0], mask=(dgrid>probe).astype(int), compactness=False)\n","    # new_max_indices = np.array(peak_local_max(dgrid, labels=reg_labels, num_peaks_per_label=1)).T\n","\n","    #* Now interpolate the region labels and sort the maxima in the correct order    \n","    sort_index = np.argsort([reg_labels[tuple(m)] for m in new_max_indices.T])\n","    new_max_indices = (new_max_indices.T[sort_index]).T\n"," \n","\n","    return reg_labels, new_max_indices.T\n","\n","def read_raspa_pdb(path_to_file):\n","\n","\n","        import numpy as np\n","\n","        f = open(path_to_file).readlines()\n","        start = np.where([\"MODEL\" in line for line in f])[0] + 2  # * Start of config\n","        ends = np.where([\"ENDMDL\" in line for line in f])[0]  # End for config\n","        cryst = np.where([\"CRYST\" in line for line in f])[0]  # box shape for the config\n","\n","        data = [f[start[i]:ends[i]] for i in range(len(start))]\n","\n","        coord = np.array([[np.array(line.split()[4:7]).astype(float) for line in d] for d in data])\n","        cell_dims = np.array([np.array(line.split())[1:].astype(float) for line in f if \"CRYST\" in line])\n","        symbols = np.array([[np.array(line.split()[2]) for line in d] for d in data])\n","\n","        output = {}\n","        output['cells'] = cell_dims\n","        output['coords'] = coord\n","        output['symbols'] = symbols\n","\n","        return output\n","\n","def interpolate_labels(regions):\n"," \n","    import numpy as np\n","    from scipy.interpolate import RegularGridInterpolator as RGI\n","    xx = np.linspace(0,  1,regions.shape[0])\n","    yy = np.linspace(0,  1,regions.shape[1])\n","    zz = np.linspace(0,  1,regions.shape[2])\n","    rl = RGI((xx, yy, zz), regions, method='nearest')\n","\n","    return rl\n","    \n","def get_fractional_coordinates(points, ase_atoms):\n"," \n","    import numpy as np\n","    import ase\n","    cell= ase.geometry.complete_cell(ase_atoms.get_cell()).T \n","    cell_inv = np.linalg.inv(cell)\n","    frac_coords = np.dot(cell_inv, points.T).T\n","    return frac_coords\n","\n","def add_maxima_to_rag(G, maxima,  maxima_radii, shape, ase_atoms):\n","\n","\n","    if len(G.nodes()) != len(maxima):\n","        raise ValueError('The number of nodes in the graph and the number of maxima do not match')\n","    else:\n","        # regionprops = get_region_props_for_regions(G.nodes())\n","        for n, m in zip(G.nodes(), maxima):\n","            G.nodes[n]['maxima'] = find_coord_from_indices(maxima[G.nodes[n]['labels'][0]-1], shape=shape, ase_atoms=ase_atoms) #* add the coordinates of the maxima to the node\n","            G.nodes[n]['maxima_indices'] = maxima[G.nodes[n]['labels'][0]-1] #* add the indcies of the maxima \n","            G.nodes[n]['maxima_radii'] =maxima_radii[G.nodes[n]['labels'][0]-1] #* add the indcies of the maxima \n","    return G"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Make some imports"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","import numpy as np \n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Read in the structure of LTA\n","\n","Reading in the structure of LTA that is consistent with the simulation box. Use `ase.visualize.view` to view the structure."]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Public\\anaconda3\\envs\\jl_env\\lib\\site-packages\\ase\\io\\cif.py:401: UserWarning: crystal system 'triclinic' is not interpreted for space group Spacegroup(1, setting=1). This may result in wrong setting!\n","  warnings.warn(\n"]}],"source":["# import mofography as mgr\n","from ase.io import read\n","mat_atoms = read('./LTA96.cif')\n","# mat_atoms = read('./Framework_0_initial_1_1_1_P1.cif')\n","\n","\n","# let's update with covalent bonds and add some extra bonds across the \n","# # periodic boundaries\n","# mat_updated = mgr.update_with_covalent_bonds(mat_atoms)\n","\n","# # Detect the bonds in the framework, this maybe useful for plotting\n","# bonds = mat_updated.info['covalent_bonds']\n","# bond_orders = mat_updated.info['covalent_bond_orders']\n","\n","# # make a structure graph, this is useful for plotting a part of of the \n","# # framework in the future\n","# mat_graph = mgr.sgraph_from_atoms(mat_updated)\n","\n","\n","# # frame traces \n","# # in the line below rings refer to the aromatic rings only\n","# frame_traces= mgr.plot_framework_as_mesh(mat_updated, show_rings= True, stylize =True)#, \n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute the distance grid\n","\n","Here we are computing a 0.25 A distance grid on the LTA unit cell"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee35009bb74246ddb1dfaf545cd93bd3","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Public\\anaconda3\\envs\\jl_env\\lib\\site-packages\\ase\\utils\\__init__.py:62: FutureWarning: Please use atoms.cell.cellpar() instead\n","  warnings.warn(warning)\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"}],"source":["spacing=0.25\n","# import mofography as mgr\n","from ase.data import covalent_radii \n","\n","radii = covalent_radii[mat_atoms.get_atomic_numbers()] \n","grid = dgrid_from_atoms(mat_atoms, radii=radii, spacing =spacing, block_size=20000)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute the region segmentation\n","\n","Breaking down the void volume into pockets with a really thin mask over the framework atoms"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Arun Gopalan\\AppData\\Local\\Temp\\ipykernel_25356\\2623633226.py:437: UserWarning: Some maxima excluded as they fell inside the mask_thickness\n","  warnings.warn('Some maxima excluded as they fell inside the mask_thickness')\n"]}],"source":["regions, maxima = regions_from_dgrid(grid.compute(), mask_thickness=0.5)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Detect connections and windows\n","Detect the connectivity of pockets and the windows between them in LTA"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c680d2d5438b4746ab96416c72d56485","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Public\\anaconda3\\envs\\jl_env\\lib\\site-packages\\numpy\\core\\shape_base.py:121: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  ary = asanyarray(ary)\n"]}],"source":["#* Find the windows and connected regions including the windows on the walls\n","# connections = mgr.connections_from_regions(dgrid = grid.compute(), region_labels=regions, maxima=maxima, ase_atoms=mat_updated)\n","rag = get_rag_from_regions_and_grid(regions, grid.compute(),mat_atoms, maxima)\n","connections = get_connections_for_rag(rag, regions, maxima,mat_atoms, grid.compute())"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["#%% #* Extract the window coordinates from the previus output, create labels for the \n","#* cages and windows based on the cage sizes; cubical, sodalite and supercage (1.44, 3.9, 6.2 )\n","import pandas as pd\n","windows = np.vstack([c[3] for c in connections if c[1]]) # pick out the window coordinates from connections\n","cage_radii = grid.compute()[tuple(maxima.T)]\n","cage_labels = pd.cut(cage_radii, bins=[0,2,5,7], labels = [ 'cubical','sodalite', 'supercage'])\n","connected_cage_labels = [cage_labels[np.array(c[0])-1] for c in connections if c[1]]\n","window_labels2 = np.array(['-'.join((c[0],c[1])) for c in connected_cage_labels])\n","window_labels1 = np.array(['-'.join((str(c[0][0]),str(c[0][1]))) for c in connections if c[1]])\n","window_labels3 = np.array([c[2] for c in connections if c[1]])\n","window_labels = [w1+\"<br>\"+w2+'<br>'+w3 for w1,w2,w3 in zip(window_labels1, window_labels2, window_labels3)]\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Read raspa output PDB\n","\n","- Read the location of the cations\n","- Create an 3D interpolator for the identity of the regions from segmenation"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# cations = mgr.read_raspa_pdb('./Movie_LTA96_1.1.1_298.150000_0.000000_component_K_0.pdb')\n","cations = read_raspa_pdb('./Movie_LTA_my_1.1.1_303.000000_0.000000_component_Na_1.pdb')\n","# cations = mgr.read_raspa_pdb('./Movie_NaX1.0newmodel2_1.1.1_298.150000_100000.000000_component_Na_0.pdb')\n","\n","# Create an linear interpolator for the regions for assigning arbitrary points (indices) to regions\n","interpolator = interpolate_labels(regions)\n","# for config in cations['coords']:\n","#     interpolator(mgr.get_fractional_coordinates(config, mat_atoms))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Assign cations to sites"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["index = 500\n","ion_region_labels = [str(x) for x in interpolator(get_fractional_coordinates(cations['coords'][index], ase_atoms=mat_atoms)).astype(int)]\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def nearest_window_and_distance(coords, ase_atoms, windows, window_labels=None, cut_off=1.0, n_neighbors=4):\n","    \"\"\"\n","    > For each ion, find the nearest window and assign the ion to that window if the distance is less\n","    than the cut-off\n","    \n","    :param coords: the coordinates of the ions\n","    :param ase_atoms: the atoms object from ASE\n","    :param windows: a list of points in 3D space\n","    :param window_labels: a list of labels for each window. If None, then the indices of the windows are\n","    used\n","    :param cut_off: the maximum distance from the window center to the ion\n","    :param n_neighbors: the number of nearest neighbors to consider, defaults to 4 (optional)\n","    :return: The window label and the distance of the window closest to each ion as long as it is within the cutoff distance, otherwise says 'not close enough'.\n","    \"\"\"\n","    ab = AABB_on_atoms(ase_atoms, points=windows)\n","    nls = ab.query(coords, query_args=dict(mode='nearest', exclude_ii=True, num_neighbors = n_neighbors )).toNeighborList(sort_by_distance=True)\n","    point_indices = nls.point_indices.reshape(-1,n_neighbors)\n","    distances = nls.distances.reshape(-1, n_neighbors)\n","    if window_labels is None:\n","        window_labels = list(range(len(windows)))   \n","    return np.vstack([[window_labels[i], d] if d <= cut_off else ['not_close_enough', d] for i,d in zip(point_indices[:,0], distances[:,0]) ]).astype('object')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Assign cations ot nearest windows"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# this is for all the cations in all the snapshots\n","labeled_ions =nearest_window_and_distance(np.vstack(cations['coords']), ase_atoms=mat_atoms, windows=windows, window_labels=window_labels, cut_off=1.5)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Print the average cation fraction at each site\n","\n","Cations are assigned ot the nearest window based on the types of the adjoining cages, only if the distance to the window is less than 1 A. otherwise, it is said to be `not_close_enough`. "]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fraction too far from windows: 0.13234375\n","Fraction near supercage-supercage windows: 0.18498263888888888\n","Fraction near sodalite-supercage windows: 0.6657465277777778\n","Fraction near sodalite-cubical windows: 0.0\n","Fraction near supercage-cubical windows: 0.016927083333333332\n"]}],"source":["#* Print out the important ion fractions\n","print('Fraction too far from windows: {0}'.format(np.mean(np.mean(labeled_ions[:,0]=='not_close_enough'))))\n","#* Fraction of ions near the supercage-supercage windows\n","print('Fraction near supercage-supercage windows: {0}'.format(np.mean([('supercage-supercage' in l) for l in labeled_ions[:,0]])))\n","print('Fraction near sodalite-supercage windows: {0}'.format(np.mean([np.logical_or('sodalite-supercage' in l, 'supercage-sodalite' in l) for l in labeled_ions[:,0]])))\n","print('Fraction near sodalite-cubical windows: {0}'.format(np.mean([np.logical_or('sodalite-cubical' in l, 'cubical-sodalite' in l) for l in labeled_ions[:,0]])))\n","print('Fraction near supercage-cubical windows: {0}'.format(np.mean([np.logical_or('supercage-cubical' in l, 'cubical-supercage' in l) for l in labeled_ions[:,0]])))\n","# np.mean([('supercage-supercage' in l) for l in labeled_ions[:,0]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"jl_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1fddfe1bc5b9fdf1a24272cfa944952943c902b9ad38442d4a2b076bbe8e62df"}}},"nbformat":4,"nbformat_minor":2}
